<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.3.0 -->
<title>Defining a Model | blog</title>
<meta property="og:title" content="Defining a Model" />
<meta property="og:locale" content="en_US" />
<link rel="canonical" href="http://localhost:4000/posts/tf_start_to_finish/model.html" />
<meta property="og:url" content="http://localhost:4000/posts/tf_start_to_finish/model.html" />
<meta property="og:site_name" content="blog" />
<script type="application/ld+json">
{"name":null,"description":null,"url":"http://localhost:4000/posts/tf_start_to_finish/model.html","headline":"Defining a Model","dateModified":null,"datePublished":null,"sameAs":null,"@type":"WebPage","author":null,"image":null,"publisher":null,"mainEntityOfPage":null,"@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="/assets/css/style.css?v=9922c528780159f2c5848e6ab49503406ffe6c8d">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name"><a href='/' style="color:white; text-decoration:none">TensorBits</a></h1>
      <h2 class="project-tagline"></h2>
      
    </section>

    <section class="main-content">
      <h1 id="part-4-defining-a-model">Part 4: Defining a Model</h1>

<div style="text-align: center">
    <a href="https://www.tensorflow.org/programmers_guide/estimators" target="_blank">TensorFlow Programmer's Guide -  Estimator</a><br />
    <a href="https://www.tensorflow.org/extend/estimators" target="_blank">TensorFlow Extend - Estimators</a><br />
</div>

<p>Now for the part you’ve all been waiting for - defining a model.</p>

<p>Note that this is a very baseline model working with an extremely limited amount of training data so don’t expect to see state of the art results.</p>

<p>To build the model we are going to use one of TensorFlow’s new (as of version 1.1) <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator" target="_blank">Estimator API</a>.  Estimators accept as arguments the things we do want to handle ourselves:</p>
<ul>
  <li>a <code class="highlighter-rouge">model_fn</code> that defines the model’s logic</li>
  <li>a dictionary of hyperparameters that define things like the learning rate, dropout rate, etc.)</li>
</ul>

<p>and abstracts away the things that should be automatic:</p>
<ul>
  <li>writing TensorBoard logs</li>
  <li>saving checkpoints</li>
  <li>looping over batches of data</li>
  <li>exporting a servable model</li>
</ul>

<p>To create an Estimator we follow these steps:</p>
<ol>
  <li>Define the Estimator object</li>
  <li>Construct the model’s logic in a function <code class="highlighter-rouge">model_fn</code></li>
  <li>Set parameters such as <code class="highlighter-rouge">model_dir</code>, <code class="highlighter-rouge">config</code>, and <code class="highlighter-rouge">params</code></li>
</ol>

<p><span class="protip"><b>Tip: </b>I like to keep this model code in a separate python file, let’s call it <code class="highlighter-rouge">model.py</code>, that is imported from the main training script.  Since research usually involves trying several architecture variants, separate model files help keep all the subtle differences in order.  Later, we will see how to copy over the exact code that is used for each training run to make completely reproducible results.</span></p>

<p>To follow along with the running example, create a new file called <code class="highlighter-rouge">model.py</code>.</p>

<h2 id="defining-the-estimator-object">Defining the Estimator object</h2>
<p>Defining the Estimator object is simple once the <code class="highlighter-rouge">model_fn</code>, <code class="highlighter-rouge">model_dir</code>, <code class="highlighter-rouge">config</code>, and <code class="highlighter-rouge">params</code> are defined - we will do this below.</p>

<p><span class="protip"><b>Tip: </b>When an Estimator is initialized, it looks in <code class="highlighter-rouge">model_dir</code> and uses the latest saved checkpoint if it exists.   If there are no saved checkpoints in <code class="highlighter-rouge">model_dir</code> a new model will be instantiated.  If <code class="highlighter-rouge">model_dir</code> is <code class="highlighter-rouge">None</code> and also not defined in <code class="highlighter-rouge">config</code> a temporary directory will be used.  Re-loading a trained model is as simple as passing in <code class="highlighter-rouge">model_dir</code> as the path to your saved model.  The most confusing thing is how to retreive the <code class="highlighter-rouge">model_fn</code> later on to load back in together.  We will see how to do this below.</span></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>################################
###   Inside code/train.py   ###
################################

estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir=model_dir, config=config, params=params)
</code></pre></div></div>

<h2 id="constructing-the-model-function">Constructing the Model Function</h2>
<p>Now, let’s define the core logic of the model, <code class="highlighter-rouge">model_fn</code>.</p>

<p>This function is called when:</p>
<ul>
  <li><b>Predicting:</b> predictions are computed and then immediately returned</li>
  <li><b>Evaluating:</b> predictions are made and evaluation metrics are computed but no step is taken with the optimizer</li>
  <li><b>Training:</b> predictions are made, evaluation metrics are computed, and optimization is performed</li>
</ul>

<p>For all projects we do this in a function with the following signature:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'''
Defines the model function passed into tf.estimator.  
This function defines the computational logic for the model.

Implementation:
    1. Define the model's computations with TensorFlow operations
    2. Generate predictions and return a prediction EstimatorSpec
    3. Define the loss function for training and evaluation
    4. Define the training operation and optimizer
    5. Return loss, train_op, eval_metric_ops in an EstimatorSpec

    Inputs:
        features: A dict containing the features passed to the model via input_fn
        labels: A Tensor containing the labels passed to the model via input_fn
        mode: One of the following tf.estimator.ModeKeys string values indicating
               the context in which the model_fn was invoked 
                  - tf.estimator.ModeKeys.TRAIN ---&gt; model.train()
                  - tf.estimator.ModeKeys.EVAL, ---&gt; model.evaluate()
                  - tf.estimator.ModeKeys.PREDICT -&gt; model.predict()

    Outputs:
        tf.EstimatorSpec that defines the model in different modes.
'''
def model_fn(features, labels, mode, params):
    # 1. Define model structure
    
    # ...
    # convolutions, denses, and batch norms, oh my!
    # ...

    # 2. Generate predictions
    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {'output_str': output_var} # alter this dictionary for your model
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
    
    # 3. Define the loss functions
    loss = ...
    
    # 3.1 Additional metrics for monitoring
    eval_metric_ops = {"rmse": tf.metrics.root_mean_squared_error(
          tf.cast(labels, tf.float64), output)}
    
    # 4. Define optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate=params['learning_rate'])
    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
    
    # 5. Return training/evaluation EstimatorSpec
    return EstimatorSpec(mode, predictions, loss, train_op, eval_metric_ops)
    
</code></pre></div></div>

<p><span class="warning"><b>Warning:</b> The main gotchya in here is making sure to put your logic in the correct order.  In predict mode you don’t have access to the label so you should make predictions first and then return.  Then, after where you will return in predict mode, you can define the loss function and optimizer.</span></p>

<p>Here is a simple convolutional neural network for our running example:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>################################
###   Inside code/model.py   ###
################################

def model_fn(features, labels, mode, params):
    # 1. Define model structure
    for l in range(params.num_layers):
        lparams = params.layers[l]
        if l == 0:
            h = features['image']
        elif lparams['type'] == 'fc' and len(h.get_shape().as_list()) != 2:
            h = tf.contrib.layers.flatten(h)
        if lparams['type'] == 'conv':
            h = tf.contrib.layers.conv2d(h, lparams['num_outputs'], lparams['kernel_size'], lparams['stride'], activation_fn=lparams['activation'], weights_regularizer=lparams['regularizer'])
        elif lparams['type'] == 'pool':
            h = tf.contrib.layers.max_pool2d(h, lparams['kernel_size'], lparams['stride'])
        elif lparams['type'] == 'fc':
            h = tf.contrib.layers.fully_connected(h, lparams['num_outputs'], activation_fn=lparams['activation'])

    # 2. Generate predictions
    if mode == tf.estimator.ModeKeys.PREDICT:
        predictions = {'output': h}
        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)
    
    # 3. Define the loss functions
    loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=h))
    
    # 3.1 Additional metrics for monitoring
    eval_metric_ops = {"accuracy": tf.metrics.accuracy(
          labels=labels, predictions=tf.argmax(h, axis=-1))}
    
    # 4. Define optimizer
    optimizer = tf.train.AdamOptimizer(learning_rate=params.learning_rate)
    train_op = optimizer.minimize(loss=loss, global_step=tf.train.get_global_step())
    
    # 5. Return training/evaluation EstimatorSpec
    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op, eval_metric_ops=eval_metric_ops)
</code></pre></div></div>

<h2 id="setting-additional-parameters">Setting Additional Parameters</h2>
<p>Before we’re done, we need to set a few last variables.</p>

<ul>
  <li><code class="highlighter-rouge">params</code>: a dictionary of model hyperparameters that is accessible in <code class="highlighter-rouge">model_fn</code></li>
  <li><code class="highlighter-rouge">config</code>: a tf.estimator.RunConfig object of runtime parameters</li>
  <li><code class="highlighter-rouge">model_dir</code>: the output directory where Estimator will write summary statistics, training checkpoints, and the graph structure</li>
</ul>

<h3 id="params">params</h3>
<p>You may have noticed in the running example <code class="highlighter-rouge">model_fn</code> that several arguments being pulled from the <code class="highlighter-rouge">params</code> dictionary.  This is a perfect place to store things like the learning rate for your optimizer, the number of layers in part of your network, the number of units in a layer, etc.</p>

<p>I also like defining <code class="highlighter-rouge">params</code> in <code class="highlighter-rouge">model.py</code> since the parameters are logically connected to the model logic and to keep the main training script clean.</p>

<p>For the running example let’s closely follow <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">AlexNet</a>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>################################
###   Inside code/model.py   ###
################################

params = tf.contrib.training.HParams(
    layers = [
        {'type': 'conv', 'num_outputs' : 96, 'kernel_size' : 11, 'stride' : 4, 'activation' : tf.nn.relu, 'regularizer' : tf.nn.l2_loss}, 
        {'type': 'pool', 'kernel_size' : 3, 'stride' : 2},
        {'type': 'conv', 'num_outputs' : 256, 'kernel_size' : 5, 'stride' : 1, 'activation' : tf.nn.relu, 'regularizer' : tf.nn.l2_loss},
        {'type': 'pool', 'kernel_size' : 3, 'stride' : 2},
        {'type': 'conv', 'num_outputs' : 384, 'kernel_size' : 3, 'stride' : 1, 'activation' : tf.nn.relu, 'regularizer' : tf.nn.l2_loss},
        {'type': 'conv', 'num_outputs' : 384, 'kernel_size' : 3, 'stride' : 1, 'activation' : tf.nn.relu, 'regularizer' : tf.nn.l2_loss},
        {'type': 'conv', 'num_outputs' : 256, 'kernel_size' : 3, 'stride' : 1, 'activation' : tf.nn.relu, 'regularizer' : tf.nn.l2_loss},
        {'type': 'pool', 'kernel_size' : 3, 'stride' : 2},
        {'type': 'fc', 'num_outputs' : 4096, 'activation' : tf.nn.relu},
        {'type': 'fc', 'num_outputs' : 2048, 'activation' : tf.nn.relu},
        {'type': 'fc', 'num_outputs' : 50, 'activation' : None}
    ],
    learning_rate = 0.001,
    train_epochs = 30,
    batch_size = 32,
    image_height = 300,
    image_width = 200,
    image_depth = 3
)
params.add_hparam('num_layers', len(params.layers))

</code></pre></div></div>

<h3 id="config">config</h3>
<p>Not to be confused with <code class="highlighter-rouge">params</code>, <code class="highlighter-rouge">config</code> is a <a href="https://www.tensorflow.org/api_docs/python/tf/estimator/RunConfig" target="_blank">tf.estimator.RunConfig</a> object that contains parameters that affect the Estimator while it is running such as <code class="highlighter-rouge">tf_random_seed</code>, <code class="highlighter-rouge">save_summary_steps</code>, <code class="highlighter-rouge">keep_checkpoint_max</code>, etc.  Passing config to the Estimator is optional but particularly helpful for monitoring training progress.</p>

<p>For the running example let’s use the following to get more frequenet summary statistics written to TensorBoard.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>################################
###   Inside code/model.py   ###
################################
config = tf.estimator.RunConfig(
    tf_random_seed=0,
    save_checkpoints_steps=250,
    save_checkpoints_secs=None,
    save_summary_steps=10,
)
</code></pre></div></div>

<h3 id="model_dir">model_dir</h3>
<p>This one is easy - pick some output directory where you want data related to training this Estimator to be saved.</p>

<p>For me, these are all in a results directory &lt;project&gt;/results/.  Make sure to add <code class="highlighter-rouge">results/</code> to your <code class="highlighter-rouge">.gitignore</code>.</p>

<p>I simply name the output directory by a timestamp of when the script is run.  You might modify this when testing different versions of a model (&lt;project&gt;/results/v1/…, &lt;project&gt;/results/v2/…, etc).</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>######################################
###   Inside code/train.py   ###
######################################

import time, datetime

ts = time.time()
timestamp = datetime.datetime.fromtimestamp(ts).strftime('%Y-%m-%d_%H-%M-%S')

model_dir_base = '../results/'
model_dir = model_dir_base + timestamp
</code></pre></div></div>

<h2 id="putting-it-all-together">Putting it All Together</h2>
<p>That’s it!  The imports in <code class="highlighter-rouge">train.py</code> will pick up <code class="highlighter-rouge">model_fn</code>, <code class="highlighter-rouge">model_dir</code>, <code class="highlighter-rouge">config</code>, and <code class="highlighter-rouge">params</code> and define the estimator.</p>

<h2 id="improving-reproducibility">Improving Reproducibility</h2>
<p>For better reproducibility I use the following lines to copy over the main training script and the model file that defines the Estimator.  This makes everything much more straight-forward when comparing several slightly varying architectures. Technically you can look up the exact architecture of the model you ran in the <em>Graph</em> tab of TensorBoard, but I’ll take the bet you’d rather take a quick peek at the python file you wrote than dig 4 levels into the TensorBoard graph visualization.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>######################################
###   Inside code/train.py   ###
######################################

import os, shutil

# Find the path of the current running file (train script)
curr_path = os.path.realpath(__file__)
model_path = curr_path.replace('train.py', 'model.py')

# Now copy the training script and the model file to 
#   model_dir -- the same directory specified when creating the Estimator
# Note: copy over more files if there are other important dependencies.
if not os.path.exists(model_dir_base):
    os.makedirs(model_dir_base)

os.mkdir(model_dir)
shutil.copy(curr_path, model_dir)
shutil.copy(model_path, model_dir)

</code></pre></div></div>

<p><span class="protip"><b>Tip: </b>If you are using Jupyter Notebooks (which you should be!), calling <code class="highlighter-rouge">tf.reset_default_graph()</code> before initializing your model is a good practice.  Doing this avoids creating extra variables and naming confusions.  One of your cells may look like: </span></p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tf.reset_default_graph()
estimator = tf.estimator.Estimator(...)
</code></pre></div></div>

<p>And that’s it!  The data is ready, the model is fully defined, and we are ready to start training.</p>

<p><span class="example"><b>Running Example: </b>here are the complete (up to this point) <a href="code/train_part4.py">train.py file</a> and <a href="code/model.py">model.py file</a>.</span></p>

<hr />

<h2 id="continue-reading">Continue Reading</h2>

<p><button onclick="location.href='traineval'" class="continue-links">Continue to Part 5</button>
In Part 5 we will train and evaluate the Estimator.</p>

<hr />

<div style="text-align: center;">
    <button onclick="location.href='https://crosleythomas.github.io/blog/'" class="continue-links" target="_blank">Blog</button>
    <button onclick="location.href='introduction'" class="continue-links">Introduction</button>
    <button onclick="location.href='setup'" class="continue-links">Part 1: Setup</button>
    <button onclick="location.href='dataprep'" class="continue-links">Part 2: Preparing Data</button>
    <button onclick="location.href='dataload'" class="continue-links">Part 3: Consuming Data</button>
    <button onclick="location.href='model'" class="continue-links">Part 4: Defining a Model</button>
    <button onclick="location.href='traineval'" class="continue-links">Part 5: Training and Evaluating</button>
    <button onclick="location.href='export'" class="continue-links">Part 6: Exporting, Testing, and Deploying</button>
    <button onclick="location.href='summary'" class="continue-links">Part 7: All Together Now</button>
    <button onclick="location.href='references'" class="continue-links">Part 8: Furthur Reading and References</button>
</div>


      <footer class="site-footer">
        <div style="text-align: center">
          <span class="site-social-media">
            
              <a href="https://twitter.com/crosleythomas" style="padding:10px 20px" target="_blank">
                <i class="fa fa-twitter"></i> Twitter
              </a>
            
            
            
              <a href="https://github.com/crosleythomas" style="padding:10px 20px" target="_blank">
                <i class="fa fa-github"></i> GitHub
              </a>
            

            
              <a href="https://linkedin.com/in/crosleythomas" style="padding:10px 20px" target="_blank">
                <i class="fa fa-linkedin"></i> LinkedIn
              </a>
            

            

            
          </span>
          <br><br><br>
        </div>


        
          <span class="site-footer-owner"><a href="http://github.com/crosleythomas/blog">Blog</a> is maintained by <a href="http://github.com/crosleythomas">crosleythomas</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </section>

    
  </body>
</html>